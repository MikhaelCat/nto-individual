import pandas as pd
import csv
import chardet
import re
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from .config import DATA_DIR

def detect_file_format(file_path):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —á—Ç–µ–Ω–∏—è CSV
    """
    print(f"  –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞: {file_path.name}")
    
    # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    sample_lines = []
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        for _ in range(10):
            line = f.readline()
            if not line:
                break
            sample_lines.append(line.strip())
    
    if not sample_lines:
        raise ValueError("–§–∞–π–ª –ø—É—Å—Ç–æ–π")
    
    print(f"    –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏: '{sample_lines[0]}'")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
    possible_separators = [';', ',', '\t', '|']
    separator_counts = {}
    
    for sep in possible_separators:
        counts = [line.count(sep) for line in sample_lines]
        separator_counts[sep] = min(counts) if counts else 0
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–π
    detected_sep = max(separator_counts, key=separator_counts.get)
    sep_count = separator_counts[detected_sep]
    
    print(f"    –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{detected_sep}' (–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è {sep_count} —Ä–∞–∑ –≤ —Å—Ç—Ä–æ–∫–µ)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–∞–≤—ã—á–µ–∫
    quote_count = sum(1 for line in sample_lines for char in line if char == '"')
    use_quotes = quote_count > 10  # –ï—Å–ª–∏ –º–Ω–æ–≥–æ –∫–∞–≤—ã—á–µ–∫, –≤–µ—Ä–æ—è—Ç–Ω–æ –µ—Å—Ç—å —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
    
    print(f"    –ö–∞–≤—ã—á–µ–∫ –≤ –æ–±—Ä–∞–∑—Ü–µ: {quote_count} -> {'–∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ' if use_quotes else '–±–µ–∑ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è'}")
    
    return {
        'sep': detected_sep,
        'quotechar': '"' if use_quotes else None,
        'quoting': csv.QUOTE_MINIMAL if use_quotes else csv.QUOTE_NONE
    }

def safe_read_csv(file_path):
    """–ù–∞–¥–µ–∂–Ω–æ–µ —á—Ç–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º —Ñ–æ—Ä–º–∞—Ç–∞"""
    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {file_path.name}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞
    format_params = detect_file_format(file_path)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
    try:
        with open(file_path, 'rb') as f:
            encoding_result = chardet.detect(f.read(100000))
        encoding = encoding_result['encoding'] or 'utf-8'
        print(f"    –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞: {encoding} (confidence: {encoding_result['confidence']:.2f})")
    except:
        encoding = 'utf-8'
        print(f"    –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {encoding}")
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    attempts = [
        # –ü–æ–ø—ã—Ç–∫–∞ 1: C-–¥–≤–∏–∂–æ–∫ —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        {
            'engine': 'c',
            'sep': format_params['sep'],
            'quotechar': format_params['quotechar'],
            'quoting': format_params['quoting'],
            'encoding': encoding,
            'on_bad_lines': 'warn',
            'low_memory': False
        },
        # –ü–æ–ø—ã—Ç–∫–∞ 2: Python-–¥–≤–∏–∂–æ–∫ (–±–æ–ª–µ–µ –≥–∏–±–∫–∏–π)
        {
            'engine': 'python',
            'sep': format_params['sep'],
            'quotechar': format_params['quotechar'],
            'quoting': format_params['quoting'],
            'encoding': encoding,
            'on_bad_lines': 'skip'
        },
        # –ü–æ–ø—ã—Ç–∫–∞ 3: –ë–µ–∑ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ –∫–∞–≤—ã—á–∫–∏ –º–µ—à–∞—é—Ç)
        {
            'engine': 'python',
            'sep': format_params['sep'],
            'quotechar': None,
            'quoting': csv.QUOTE_NONE,
            'encoding': encoding,
            'on_bad_lines': 'skip'
        }
    ]
    
    for i, params in enumerate(attempts, 1):
        try:
            print(f"    üîÑ –ü–æ–ø—ã—Ç–∫–∞ {i}: engine={params['engine']}, sep='{params['sep']}', encoding={encoding}")
            
            # –£–¥–∞–ª—è–µ–º None-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            clean_params = {k: v for k, v in params.items() if v is not None}
            
            df = pd.read_csv(file_path, **clean_params)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –±–æ–ª—å—à–µ 1 —Å—Ç–æ–ª–±—Ü–∞
            if len(df.columns) <= 1:
                raise ValueError(f"–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ –æ–¥–∏–Ω —Å—Ç–æ–ª–±–µ—Ü. –°—Ç–æ–ª–±—Ü—ã: {df.columns.tolist()}")
            
            print(f"    ‚úÖ –£—Å–ø–µ—à–Ω–æ! –°—Ç—Ä–æ–∫: {len(df):,}, –°—Ç–æ–ª–±—Ü–æ–≤: {len(df.columns)}")
            print(f"       –°—Ç–æ–ª–±—Ü—ã: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}")
            
            # –í—ã–≤–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if not df.empty:
                print("       –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
                print(df.head(2).to_string(index=False))
            
            return df
            
        except Exception as e:
            print(f"    ‚ùå –ü–æ–ø—ã—Ç–∫–∞ {i} –Ω–µ —É–¥–∞–ª–∞—Å—å: {str(e)[:100]}...")
            continue
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞: —Ä—É—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫
    print("    ‚ö†Ô∏è –í—Å–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å. –ü—Ä–æ–±—É–µ–º —Ä—É—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ...")
    try:
        with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
            lines = f.readlines()
        
        if not lines:
            raise ValueError("–§–∞–π–ª –ø—É—Å—Ç–æ–π")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –¥–∞–Ω–Ω—ã–µ
        headers = lines[0].strip().split(format_params['sep'])
        data_rows = []
        
        for line in lines[1:]:
            row = line.strip().split(format_params['sep'])
            if len(row) == len(headers):
                data_rows.append(row)
        
        df = pd.DataFrame(data_rows, columns=headers)
        print(f"    ‚úÖ –†—É—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞! –°—Ç—Ä–æ–∫: {len(df):,}, –°—Ç–æ–ª–±—Ü–æ–≤: {len(df.columns)}")
        return df
        
    except Exception as e:
        print(f"    ‚ùå –†—É—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {str(e)[:100]}...")
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª {file_path.name} –Ω–∏ –æ–¥–Ω–∏–º –∏–∑ –º–µ—Ç–æ–¥–æ–≤")

def load_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–ª–æ–≤–∞—Ä—å —Å –Ω–∞–¥–µ–∂–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
    print("\nüìö –ó–ê–ì–†–£–ó–ö–ê –ò –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•")
    data = {}
    
    # –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    files_to_load = {
        "train": "train.csv",
        "test": "test.csv",
        "books": "books.csv",
        "users": "users.csv",
        "genres": "genres.csv",
        "book_genres": "book_genres.csv",
        "book_descriptions": "book_descriptions.csv"
    }
    
    for key, filename in files_to_load.items():
        file_path = DATA_DIR / filename
        
        if not file_path.exists():
            raise FileNotFoundError(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path.absolute()}")
        
        print(f"\n{'='*50}")
        print(f"üìÑ –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–æ–º: {filename}")
        print(f"{'='*50}")
        
        try:
            data[key] = safe_read_csv(file_path)
        except Exception as e:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {filename}:")
            print(f"    {str(e)}")
            print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ:")
            print("1. –û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ä–µ–¥–∞–∫—Ç–æ—Ä–µ (Notepad++, VS Code)")
            print("2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π ';')")
            print("3. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏")
            print("4. –£–¥–∞–ª–∏—Ç–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞")
            print("5. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Ñ–∞–π–ª –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ UTF-8 –±–µ–∑ BOM")
            raise
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º train —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    print(f"\n{'='*50}")
    print("üßπ –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –û–ë–£–ß–ê–Æ–©–ò–• –î–ê–ù–ù–´–•")
    print(f"{'='*50}")
    
    print("  –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –≤ train.csv...")
    required_columns = ['user_id', 'book_id', 'rating', 'has_read']
    missing_cols = [col for col in required_columns if col not in data["train"].columns]
    
    if missing_cols:
        print(f"‚ùå –û–¢–°–£–¢–°–¢–í–£–Æ–©–ò–ï –°–¢–û–õ–ë–¶–´: {', '.join(missing_cols)}")
        print("  –°—Ç–æ–ª–±—Ü—ã –≤ —Ñ–∞–π–ª–µ:", ', '.join(data["train"].columns))
        raise KeyError(f"–í train.csv –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {missing_cols}")
    
    initial_count = len(data["train"])
    data["train"] = data["train"][data["train"]["has_read"] == 1].copy()
    filtered_count = len(data["train"])
    
    print(f"  –î–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {initial_count:,} —Å—Ç—Ä–æ–∫")
    print(f"  –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ has_read=1): {filtered_count:,} —Å—Ç—Ä–æ–∫")
    print(f"  –£–¥–∞–ª–µ–Ω–æ: {initial_count - filtered_count:,} —Å—Ç—Ä–æ–∫ ({(initial_count - filtered_count)/initial_count:.1%})")
    
    print(f"\n{'='*50}")
    print("‚úÖ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"{'='*50}")
    
    return data